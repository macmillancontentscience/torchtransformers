---
title: "Textual Entailment"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Textual Entailment}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  eval = FALSE
)
```

```{r setup}
library(torch)
library(torchtransformers)
library(luz)

library(dlr)
library(dplyr)
```

PROBABLY CUT THIS DOWN BUT I WROTE IT ALL OUT IN CASE WE WANT IT ALL.

Textual entailment is a common NLP task, and is included in the [GLUE](https://gluebenchmark.com/) and [SuperGLUE](https://super.gluebenchmark.com/) NLP benchmarks.
The task consists of two pieces of text, a `premise` and a `hypothesis`.
For example, this is a premise/hypothesis pair from the [MultiNLI dataset](https://cims.nyu.edu/~sbowman/multinli/) (MNLI, described in more detail below):

- **Premise:** "Here you'll see a shrunken head, a two-headed goat, and a statue of Marilyn Monroe made of shredded money, among other curiosities."
- **Hypothesis:** "One of the curiosities is a two-headed goat."

In this case, the premise *entails* the hypothesis.
This means that the hypothesis follows from the premise.

In contrast, this is another premise/hypothesis pair from the same dataset:

- **Premise:** "There is also an archaeological museum that displays older relics, including examples of Mycenaean pottery."
- **Hypothesis:** "The museum is completely empty and doesn't have anything in it."

In this case, the premise *contradicts* the hypothesis.
The premise lists things that are displayed in the museum, while the hypothesis asserts that the museum is empty.

Finally, this is another premise/hypothesis pair from MNLI:

- **Premise::** "At the heart of the sanctuary, a small granite shrine once held the sacred barque of Horus himself."
- **Hypothesis:** "Horus is a god."

While Horus was an Egyptian god, the premise doesn't mention that, so the premise neither entails nor contradicts the hypothesis. 
This pair is said to be **neutral.**

In this vignette, we'll use the MNLI dataset to fine-tune a BERT model for an entailment task.

## The MNLI Dataset

The Multi-Genre Natural Language Inference (MultiNLI or MNLI) corpus was described in [A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference](https://aclanthology.org/N18-1101) (Williams et al., NAACL 2018).
It includes 433k premise-hypothesis pairs, annotated with entailment information.
The premises are divided into 10 genres.
Five of the genres ("fiction", "government", "slate", "telephone", and "travel") are included in the training dataset,
and the other five genres ("facetoface", "letters", "nineeleven", "oup", and "verbatim") are not.

The data are subdivided into five datasets:

- train.tsv (392,702 observations from five genres)
- dev_matched.tsv (9,815 observations from the five training genres)
- dev_mismatched.tsv (9,832 observations from the other five genres)
- test_matched.tsv (9,796 observations from the five training genres, no labels)
- test_mismatched.tsv (9,847 observations from the other five genres, no labels)

The test sets are for scoring your model on Kaggle, so we'll skip those.

We'll train our model using `train.tsv`, and test it using `dev_matched.tsv` and `dev_mismatched.tsv`.

We will eventually add a function to this package to download and prepare these datasets, but for now we'll do so manually.

```{r download}
# Set up a processor function for {dlr} to load the data. Soon (maybe before
# this vignette is merged) we'll export a function that takes this data all the
# way through to a torch dataset.
process_mnli <- function(source_file) {
  dataset_names <- c(
    "train",
    "dev_matched",
    "dev_mismatched",
    "test_matched",
    "test_mismatched"
  )
  # Also make those the names so purrr uses them.
  names(dataset_names) <- dataset_names
  
  mnli_tibbles <- purrr::map(
    dataset_names,
    function(this_dataset) {
      # We specify column types to make sure things come in as we expect.
      column_spec <- dplyr::case_when(
        stringr::str_starts(this_dataset, "dev_") ~ "iiccccccccffffff",
        stringr::str_starts(this_dataset, "test_") ~ "iiiccccccc",
        TRUE ~ "iiccccccccff"
      )
      raw_tibble <- readr::read_tsv(
        unz(source_file, fs::path("MNLI", this_dataset, ext = "tsv")),
        col_types = column_spec,
        # There are a couple lines that screw up if we include a quote
        # character.
        quote = ""
      )
      return(
        dplyr::select(
          raw_tibble,
          -.data$index,
          -.data$promptID,
          -.data$pairID,
          -dplyr::ends_with("_parse"),
          -dplyr::starts_with("label")
        )
      )
    }
  )
  
  return(mnli_tibbles)
}

# By default downloading large files often fails. Increase the timeout.
old_timeout <- options(timeout = 1000)

data_url <- "https://dl.fbaipublicfiles.com/glue/data/MNLI.zip"

mnli_tibbles <- dlr::read_or_cache(
  source_path = data_url,
  appname = "torchtransformers",
  process_f = process_mnli
)

options(old_timeout)
```

We need to set these datasets up for use with {luz}.

```{r dataset-constructor}
entailment_constructor <- torch::dataset(
  name = "entailment_dataset",
  initialize = function(premises, hypotheses, labels, n_tokens = 512) {
    
    
    # First we need to tokenize everything in the style required for BERT.
    anchor_tokenized <- tokenize_bert(
      text = df$anchor,
      n_tokens = n_tokens
    )
    positive_tokenized <- tokenize_bert(
      text = df$positive,
      n_tokens = n_tokens
    )
    negative_tokenized <- tokenize_bert(
      text = df$negative,
      n_tokens = n_tokens
    )
    
    # We supply those matrices as tensors.
    self$anchor <- torch::torch_tensor(anchor_tokenized$token_ids)
    self$positive <- torch::torch_tensor(positive_tokenized$token_ids)
    self$negative <- torch::torch_tensor(negative_tokenized$token_ids)
    
    # For this model, the token_types matrix is the same for all three types of
    # data, so we'll just return one of them.
    self$token_types <- torch::torch_tensor(anchor_tokenized$token_type_ids)
  },
  # We extract subsets of this data using an index.
  .getitem = function(index) {
    list(
      list(
        anchor = list(
          token_ids = self$anchor[index, ],
          token_type_ids = self$token_types[index, ]
        ), 
        positive = list(
          token_ids = self$positive[index, ],
          token_type_ids = self$token_types[index, ]
        ), 
        negative = list(
          token_ids = self$negative[index, ],
          token_type_ids = self$token_types[index, ]
        )
      ),
      list() # No target.
    )
  },
  # The dataset also needs a method to determine the length of the entire
  # dataset.
  .length = function() {
    dim(self$anchor)[[1]]
  }
) 
```
