---
title: "Triplet Loss"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Triplet Loss}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

<!-- 
Copyright 2022 Bedford Freeman & Worth Pub Grp LLC DBA Macmillan Learning.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  eval = FALSE,
  comment = "#>"
)
```

```{r setup}
library(torch)
library(torchtransformers)
library(luz)

library(dlr)
library(dplyr)
library(jsonlite)
library(wordpiece)
library(wordpiece.data)
```

This vignette will walk through the process of training a "triplet" model to 
produce sentence embeddings for some task. 

Each training example for this type of model consists of three pieces of input text: the anchor, the positive example, and the negative example. 
The training loss for the model will be the _difference_ between

- The distance from the anchor to the negative example, and
- the distance from the anchor to the positive example. 

The loss is minimized during training, so the model learns to put the anchor closer to the positive example than to the negative example in embedding space.

After training, the hope is that the model has learned to make useful representations for the input examples. 
The model can then be used to generate sentence-level embedding vectors for any input sentences.


## Get the data

The data we will use for this vignette is derived from NLI datasets, and can be
obtained [online](https://www.sbert.net/examples/training/paraphrases/README.html#datasets).
We use the `read_or_cache()` function from `{dlr}` to avoid repeatedly processing the same dataset.

```{r download}
# dlr uses a processor function that takes the path to a temp file as its first
# argument. Here we get the data to the sort of point from which you might start
# with your own text data.
paraphrase_processor <- function(source_file) {
  return(
    jsonlite::stream_in(file(source_file)) %>% 
      dplyr::as_tibble() %>% 
      dplyr::rename(
        anchor = "V1",
        positive = "V2",
        negative = "V3"
      ) %>% 
      # The dataset contains some long, rambling sentences that we don't want
      # for this demonstration. Filter out those entire rows.
      dplyr::mutate(
        max_chars = pmax(
          nchar(anchor), nchar(positive), nchar(negative)
        )
      ) %>% 
      dplyr::filter(max_chars <= 300) %>% 
      dplyr::select(-max_chars)
  )
}

data_url <- "https://public.ukp.informatik.tu-darmstadt.de/reimers/sentence-transformers/datasets/paraphrases/AllNLI.jsonl.gz"

paraphrases <- dlr::read_or_cache(
  source_path = data_url,
  appname = "torchtransformers",
  process_f = paraphrase_processor
)

paraphrases$anchor[[1]]
# [1] "A person on a horse jumps over a broken down airplane."

paraphrases$positive[[1]]
# [2] "A person is outdoors, on a horse."                     

paraphrases$negative[[1]]
# [3] "A person is at a diner, ordering an omelette."
```

## Prepare the data for torch

To load triplet data into `{torch}`, we will need to define a custom torch `dataset` constructor.
This constructor will convert a dataframe containing vectors of anchors, positive examples, and negative examples into a dataset that can be loaded into `{torch}`.
For ease-of-use we're passing in a dataframe and assuming it is named like our dataframe.

```{r dataset-constructor}
triplet_text_dataset <- torch::dataset(
  name = "triplet_text_dataset",
  initialize = function(df, n_tokens = 128) {
    # First we need to tokenize everything in the style required for BERT.
    anchor_tokenized <- tokenize_bert(
      text = df$anchor,
      n_tokens = n_tokens
    )
    positive_tokenized <- tokenize_bert(
      text = df$positive,
      n_tokens = n_tokens
    )
    negative_tokenized <- tokenize_bert(
      text = df$negative,
      n_tokens = n_tokens
    )
    
    # We supply those matrices as tensors.
    self$anchor <- torch::torch_tensor(anchor_tokenized$token_ids)
    self$positive <- torch::torch_tensor(positive_tokenized$token_ids)
    self$negative <- torch::torch_tensor(negative_tokenized$token_ids)
    
    # For this model, the token_types matrix is the same for all three types of
    # data, so we'll just return one of them.
    self$token_types <- torch::torch_tensor(anchor_tokenized$token_type_ids)
  },
  # We extract subsets of this data using an index.
  .getitem = function(index) {
    list(
      list(
        anchor = list(
          token_ids = self$anchor[index, ],
          token_type_ids = self$token_types[index, ]
        ), 
        positive = list(
          token_ids = self$positive[index, ],
          token_type_ids = self$token_types[index, ]
        ), 
        negative = list(
          token_ids = self$negative[index, ],
          token_type_ids = self$token_types[index, ]
        )
      ),
      list() # No target.
    )
  },
  # The dataset also needs a method to determine the length of the entire
  # dataset.
  .length = function() {
    dim(self$anchor)[[1]]
  }
)
```

We use the constructor to create a training dataset.
We'll let {lux} split this dataset into training and validation.
The dataset constructor performs the tokenization, so this step will be somewhat slow.

```{r datasets}
train_ds <- triplet_text_dataset(paraphrases)
```

## Define the model

We're going to build a "triplet" model with a BERT pre-trained "spine" (specifically "bert_tiny_uncased").
We use the `make_and_load_bert` function to create a BERT module with pre-trained
weights loaded.
We use the final `[CLS]` token as the output, and then a dense layer after that to map into our paraphrase embedding space.

```{r spine-definition}
spine_with_pooling <- torch::nn_module(
  classname = "pooled_spine",
  initialize = function(model_name = "bert_tiny_uncased") {
    embedding_size <- config_bert(
      model_name = model_name, 
      parameter = "embedding_size"
    ) 
    self$bert <- make_and_load_bert(model_name)
    # After pooled BERT output, do a final dense layer.
    self$linear <- torch::nn_linear(
      in_features = embedding_size, 
      out_features = embedding_size
    )
  },
  forward = function(x) {
    output <- self$bert(
      x$token_ids, 
      x$token_type_ids
    )

    # Take the output embeddings from the final BERT layer.
    output <- output$output_embeddings
    output <- output[[length(output)]]
    # Take the [CLS] token embedding (token 1 in each example) for pooling. The
    # three dimensions are: example (length = batch_size), token (length =
    # n_tokens), embeddings (length = defined by the model type).
    output <- output[ , 1, ]
    # Apply the last dense layer to the pooled output.
    output <- self$linear(output)
    return(output)
  }
)
```

The model is that same spine used for the three different inputs.
We use `torch::nn_triplet_margin_loss` to measure the loss using the three outputs.

```{r model-definition}
triplet_text_model <- torch::nn_module(
  classname = "triplet_text",
  initialize = function(model_name = "bert_tiny_uncased") {
    # Share the same spine parameters between all three inputs.
    self$spine <- spine_with_pooling(model_name)
    self$criterion <- nn_triplet_margin_loss()
  },
  loss = function(input, ...) {
    # input contains anchor, positive, and negative, each of which is composed
    # of token_ids and token_type_ids.
    embeddings <- lapply(input, self$spine)
    loss <- self$criterion(
      embeddings$anchor,
      embeddings$positive,
      embeddings$negative
    )
    return(loss)
  }
)
```

## Fit the model

We use `{luz}` to perform the actual model fitting.
With these settings, this model gets about as good as it's going to get after a single epoch.

```{r fit}
set.seed(12345)
torch::torch_manual_seed(123456)
fitted <- triplet_text_model %>% 
  luz::setup(optimizer = torch::optim_adam) %>% 
  fit(
    train_ds, 
    epochs = 1, 
    valid_data = 0.1, 
    dataloader_options = list(batch_size = 256L)
  )
```

After a single epoch of training, the loss on the validation set is about 0.22.

## Using the model

We'll use the weights from the spine of our model to map an input into embedding space.

```{r extract-weights}
model_weights <- fitted$model$state_dict()
```

We define a simpler model that takes a single type of input, and outputs sentence vectors to map our inputs into our paraphrase embedding space.
This model uses the same parameters as the triplet model, but with a single call to the spine.

```{r vectorizer}
# The vectorizer model is the same spine that we used for the three inputs. We
# wrap it in a larger constructor so the weight names line up for easier loading.
vectorizer <- torch::nn_module(
  "vectorizer",
  initialize = function(model_name = "bert_tiny_uncased") {
    self$spine <- spine_with_pooling(model_name)
  },
  forward = function(input) {
    return(self$spine(input))
  }
)

vectorizer_model <- vectorizer()

# Load our trained weights into this model.
vectorizer_model$load_state_dict(model_weights)
```

We can use that model to map the positive paraphrases from our dataset into an embedding space.
We'll work with a subset to make sure they can all fit in RAM.

```{r paraphrase-space}
vectorizer_model$eval()

paraphrase_space <- vectorizer_model(
  list(
    token_ids = train_ds$positive[1:10000, ], 
    token_type_ids = train_ds$token_types[1:10000, ]
  )
)
```

A given anchor should, in theory, be closest to its associated paraphrase in that space.

```{r anchors-in-space}
# Map some of the anchors into that same embedding space.
anchors_in_space <- vectorizer_model(
  list(
    token_ids = train_ds$anchor[1:100, ],
    token_type_ids = train_ds$token_types[1:100, ]
  )
)

# Create a simple pairwise distance calculator.
pair_dist <- torch::nn_pairwise_distance()

# I picked out the 8th example because its result is somewhat interesting.
anchor_id <- 8
distances <- pair_dist(anchors_in_space[anchor_id, ], paraphrase_space)
closest_match <- which(
  torch::as_array(distances) == min(torch::as_array(distances))
)

paraphrases$anchor[[anchor_id]]
#> [1] "Two women who just had lunch hugging and saying goodbye."

# The actual "correct" paraphrase.
paraphrases$positive[[anchor_id]]
#> [1] "There are two woman in this picture."

# The closest paraphrase in our embedding space.
paraphrases$positive[[closest_match]]
#> [1] "A couple is having lunch"
```

While this is incorrect, you can see that the model picked up on the relationship between "two" and "a couple", and also that the sentence had something to do with having lunch.
Further refinement of this model would be necessary to make the results actually useful, but the general approach can lead to models that are useful for quickly matching a new piece of text to a large collection of known text.
If we were using this for something real, we would want a separate test set that maps to the same set of paraphrases.
