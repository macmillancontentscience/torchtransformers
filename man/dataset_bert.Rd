% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/dataset_bert.R
\name{dataset_bert}
\alias{dataset_bert}
\title{BERT Dataset}
\usage{
dataset_bert(x, y = NULL, tokenizer = tokenize_bert, n_tokens = 128L)
}
\arguments{
\item{x}{A data.frame with one or more character predictor columns.}

\item{y}{A factor of outcomes, or a data.frame with a single factor column.
Can be NULL (default).}

\item{tokenizer}{A tokenization function (signature compatible with
\code{tokenize_bert}).}

\item{n_tokens}{Integer scalar; the number of tokens expected for each
example.}
}
\value{
An initialized \code{\link[torch:dataset]{torch::dataset()}}.
}
\description{
Prepare a dataset for BERT-like models.
}
\section{Methods}{

\describe{
\item{\code{initialize}}{Initialize this dataset. This method is called when the
dataset is first created.}
\item{\code{.getitem}}{Fetch an individual predictor (and, if available, the
associated outcome). This function is called automatically by \code{{luz}}
during the fitting process.}
\item{\code{.length}}{Determine the length of the dataset (the number of rows of
predictors). Generally superseded by instead calling \code{\link[=length]{length()}}.}
}
}

