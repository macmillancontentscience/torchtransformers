% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/attention.R
\name{bert_attention}
\alias{bert_attention}
\title{Perform Attention}
\usage{
bert_attention(embedding_size, n_head, dropout = 0.1)
}
\arguments{
\item{dropout}{}
}
\description{
Takes in an input tensor (e.g. sequence of token embeddings), applies an
attention layer and layer-norms the result. Returns both the attention
weights and the attention output.
}
\details{
eventually this will inherit...
}
\section{Shape}{


Inputs:

- input: \eqn{(sequence_length, *, embedding_size)}

Output:

- \eqn{(sequence_length, *, embedding_size)}
- \eqn{(sequence_length, *, embedding_size)} # fix: attn wts
}

\examples{

}
