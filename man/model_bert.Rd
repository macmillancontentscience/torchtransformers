% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/bert.R
\name{model_bert}
\alias{model_bert}
\title{BERT Model}
\usage{
model_bert(
  embedding_size,
  intermediate_size = 4 * embedding_size,
  n_layer,
  n_head,
  hidden_dropout = 0.1,
  attention_dropout = 0.1,
  max_position_embeddings,
  vocab_size,
  token_type_vocab_size = 2L
)
}
\arguments{
\item{embedding_size}{Integer; the dimension of the embedding vectors.}

\item{intermediate_size}{Integer; size of dense layers applied after
attention mechanism.}

\item{n_layer}{Integer; the number of attention layers.}

\item{n_head}{Integer; the number of attention heads per layer.}

\item{hidden_dropout}{Numeric; the dropout probability to apply to dense
layers.}

\item{attention_dropout}{Numeric; the dropout probability to apply in
attention.}

\item{max_position_embeddings}{Integer; maximum number of tokens in each
input sequence.}

\item{vocab_size}{Integer; number of tokens in vocabulary.}

\item{token_type_vocab_size}{Integer; number of input segments that the model
will recognize. (Two for BERT models.)}
}
\description{
Construct a BERT model.
}
\section{Shape}{


  Inputs:

  With `sequence_length` <= `max_position_embeddings`:

  - token_ids: \eqn{(sequence_length, *)}

  - token_type_ids: \eqn{(sequence_length, *)}

  Output:

  - initial_embeddings: \eqn{(sequence_length, *, embedding_size)}

  - output_embeddings: list of \eqn{(sequence_length, *, embedding_size)} for
  each transformer layer.

  - attention_weights: list of \eqn{(*, n_head, sequence_length,
  sequence_length)} for each transformer layer.
}

\examples{
emb_size <- 128L
mpe <- 512L
n_head <- 4L
n_layer <- 6L
vocab_size <- 30522L
model <- model_bert(embedding_size = emb_size,
              n_layer = n_layer,
              n_head = n_head,
              max_position_embeddings = mpe,
              vocab_size = vocab_size)

n_inputs <- 2
n_token_max <- 128L
# get random "ids" for input
t_ids <- matrix(sample(2:vocab_size, size = n_token_max * n_inputs,
                      replace = TRUE),
               nrow = n_token_max, ncol = n_inputs)
ttype_ids <- matrix(rep(1L, n_token_max * n_inputs),
               nrow = n_token_max, ncol = n_inputs)
model(torch::torch_tensor(t_ids),
      torch::torch_tensor(ttype_ids))
}
