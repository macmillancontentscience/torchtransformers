% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/embeddings.R
\name{bert_embeddings}
\alias{bert_embeddings}
\title{Create BERT Embeddings}
\usage{
bert_embeddings(
  embedding_size,
  max_position_embeddings,
  vocab_size,
  token_type_vocab_size = 2L,
  hidden_dropout = 0.1
)
}
\arguments{
\item{hidden_dropout}{}
}
\description{
There are three components which are added together to give the input
embeddings in a BERT model: the embedding of the tokens themselves, the
segment ("token type") embedding, and the position (token index) embedding.
This function sets up the embedding layer for all three of these.
}
\details{
This will eventually inherit from a higher-level function.
}
\section{Shape}{


  Inputs:

  - input_ids: \eqn{(max_position_embeddings, *)}

  - token_type_ids: \eqn{(max_position_embeddings, *)}


  Output:

  - \eqn{(max_position_embeddings, *, embedding_size)}
}

\examples{
emb_size <- 3L
mpe <- 5L
vs <- 7L
n_inputs <- 2L
# get random "ids" for input
t_ids <- matrix(sample(2:vs, size = mpe*n_inputs, replace = TRUE),
                nrow = mpe, ncol = n_inputs)
ttype_ids <- matrix(rep(1L, mpe*n_inputs), nrow = mpe, ncol = n_inputs)

test_model <- bert_embeddings(embedding_size = emb_size,
                              max_position_embeddings = mpe,
                              vocab_size = vs)
test_model(torch::torch_tensor(t_ids),
           torch::torch_tensor(ttype_ids))
}
