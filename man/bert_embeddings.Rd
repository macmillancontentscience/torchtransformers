% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/embeddings.R
\name{bert_embeddings}
\alias{bert_embeddings}
\title{Create BERT Embeddings}
\usage{
bert_embeddings(
  embedding_size,
  max_position_embeddings,
  vocab_size,
  token_type_vocab_size = 2L,
  hidden_dropout = 0.1
)
}
\arguments{
\item{embedding_size}{Integer; the dimension of the embedding vectors.}

\item{max_position_embeddings}{Integer; maximum number of tokens in each
input sequence.}

\item{vocab_size}{Integer; number of tokens in vocabulary.}

\item{token_type_vocab_size}{Integer; number of input segments that the model
will recognize. (Two for BERT models.)}

\item{hidden_dropout}{Numeric; the dropout probability to apply to dense
layers.}
}
\description{
There are three components which are added together to give the input
embeddings in a BERT model: the embedding of the tokens themselves, the
segment ("token type") embedding, and the position (token index) embedding.
This function sets up the embedding layer for all three of these.
}
\section{Shape}{


With `sequence_length` <= `max_position_embeddings`:

  Inputs:

  - input_ids: \eqn{(sequence_length, *)}

  - token_type_ids: \eqn{(sequence_length, *)}


  Output:

  - \eqn{(sequence_length, *, embedding_size)}
}

\examples{
emb_size <- 3L
mpe <- 5L
vs <- 7L
n_inputs <- 2L
# get random "ids" for input
t_ids <- matrix(sample(2:vs, size = mpe * n_inputs, replace = TRUE),
                nrow = mpe, ncol = n_inputs)
ttype_ids <- matrix(rep(1L, mpe * n_inputs), nrow = mpe, ncol = n_inputs)

model <- bert_embeddings(embedding_size = emb_size,
                         max_position_embeddings = mpe,
                         vocab_size = vs)
model(torch::torch_tensor(t_ids),
      torch::torch_tensor(ttype_ids))
}
