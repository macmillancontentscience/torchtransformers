% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/tokenize.R
\name{.tokenize_bert_single}
\alias{.tokenize_bert_single}
\title{Tokenize a single vector of text}
\usage{
.tokenize_bert_single(
  text,
  n_tokens = 64L,
  simplify = TRUE,
  increment_index = TRUE,
  pad_token = "[PAD]",
  cls_token = "[CLS]",
  sep_token = "[SEP]",
  tokenizer = wordpiece::wordpiece_tokenize,
  vocab = wordpiece.data::wordpiece_vocab(),
  tokenizer_options = NULL
)
}
\arguments{
\item{text}{A character vector, or a list of length-1 character vectors.}

\item{n_tokens}{Integer scalar; the number of tokens expected for each
example.}

\item{simplify}{Logical scalar; whether to return the result as a list
(\code{FALSE}), or as a matrix. The matrix returned is currently
\code{n_tokens} rows by \code{length(text)} columns, but we plan to
transpose that in an upcoming change to the overall package API.}

\item{increment_index}{Logical; if TRUE, add 1L to all token ids to convert
from the Python-inspired 0-indexed standard to the torch 1-indexed
standard.}

\item{pad_token}{Character scalar; the token to use for padding. Must be
present in the supplied vocabulary.}

\item{cls_token}{Character scalar; the token to use at the start of each
example. Must be present in the supplied vocabulary, or \code{NULL}.}

\item{sep_token}{Character scalar; the token to use at the end of each
segment within each example. Must be present in the supplied vocabulary, or
\code{NULL}.}

\item{tokenizer}{The tokenizer function to use to break up the text. It must
have a \code{vocab} argument.}

\item{vocab}{The vocabulary to use to tokenize the text. This vocabulary must
include the \code{pad_token, cls_token, and sep_token}.}

\item{tokenizer_options}{A named list of additional arguments to pass on to
the tokenizer.}
}
\value{
A list containing a list or matrix of token ids, and a list or matrix
  of token type ids.
}
\description{
Tokenize a single vector of text
}
\keyword{internal}
