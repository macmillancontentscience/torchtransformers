% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/bert_pretrained.R
\name{.concatenate_qkv_weights}
\alias{.concatenate_qkv_weights}
\title{Concatenate Attention Weights}
\usage{
.concatenate_qkv_weights(state_dict)
}
\arguments{
\item{state_dict}{A state_dict of pretrained weights, probably loaded from a
file.}
}
\value{
The state_dict with query, key, value weights concatenated.
}
\description{
Concatenate weights to format attention parameters appropriately for loading
into BERT models. The torch attention module puts the weight/bias values for
the q,k,v tensors into a single tensor, rather than three separate ones. We
do the concatenation so that we can load into our models.
}
\keyword{internal}
