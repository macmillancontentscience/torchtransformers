% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/attention.R
\name{attention_bert}
\alias{attention_bert}
\title{BERT-Style Attention}
\usage{
attention_bert(embedding_size, n_head, attention_dropout = 0.1)
}
\arguments{
\item{embedding_size}{Integer; the dimension of the embedding vectors.}

\item{n_head}{Integer; the number of attention heads per layer.}

\item{attention_dropout}{Numeric; the dropout probability to apply in
attention.}
}
\description{
Takes in an input tensor (e.g. sequence of token embeddings), applies an
attention layer and layer-norms the result. Returns both the attention
weights and the output embeddings.
}
\section{Shape}{


Inputs:

- input: \eqn{(sequence_length, *, embedding_size)}

- optional mask: \eqn{(*, sequence_length)}

Output:

- embeddings: \eqn{(sequence_length, *, embedding_size)}

- weights: \eqn{(*, n_head, sequence_length, sequence_length)}
}

\examples{
emb_size <- 4L
seq_len <- 3L
n_head <- 2L
batch_size <- 2L

model <- attention_bert(embedding_size = emb_size,
                        n_head = n_head)
# get random values for input
input <- array(sample(-10:10,
                      size = batch_size * seq_len * emb_size,
                      replace = TRUE) / 10,
               dim = c(seq_len, batch_size, emb_size))
input <- torch::torch_tensor(input)
model(input)
}
