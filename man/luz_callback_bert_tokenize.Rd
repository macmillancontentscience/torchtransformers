% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/luz_callbacks.R
\name{luz_callback_bert_tokenize}
\alias{luz_callback_bert_tokenize}
\title{BERT Tokenization Callback}
\usage{
luz_callback_bert_tokenize(
  submodel_name = NULL,
  n_tokens = NULL,
  verbose = TRUE
)
}
\arguments{
\item{submodel_name}{An optional character scalar identifying a model inside
the main \code{\link[torch:nn_module]{torch::nn_module()}} that was built using
\code{\link[=model_bert_pretrained]{model_bert_pretrained()}}. See \code{vignette("entailment")} for an example of a
model with a submodel.}

\item{n_tokens}{An optional integer scalar indicating the number of tokens to
which the data should be tokenized. If present it must be equal to or less
than the \code{max_tokens} allowed by the pretrained model.}

\item{verbose}{A logical scalar indicating whether the callback should report
its progress (default \code{TRUE}).}
}
\description{
Data used in pretrained BERT models must be tokenized in the way the model
expects. This \code{luz_callback} checks that the incoming data is tokenized
properly, and triggers tokenization if necessary. This function should be
passed to \code{\link[luz:fit.luz_module_generator]{luz::fit.luz_module_generator()}} or
\code{\link[luz:predict.luz_module_fitted]{luz::predict.luz_module_fitted()}} via the \code{callbacks} argument, not called
directly.
}
\examples{
if (rlang::is_installed("luz")) {
  luz_callback_bert_tokenize()
  luz_callback_bert_tokenize(n_tokens = 32L)
}

}
