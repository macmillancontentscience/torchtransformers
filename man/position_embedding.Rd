% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/embeddings.R
\name{position_embedding}
\alias{position_embedding}
\title{Create Position Embeddings}
\usage{
position_embedding(embedding_size, max_position_embeddings)
}
\arguments{
\item{embedding_size}{Integer; the dimension of the embedding vectors.}

\item{max_position_embeddings}{Integer; maximum number of tokens in each
input sequence.}
}
\description{
Position embeddings are how BERT-like language models represent the order of
input tokens. Each token gets a position embedding vector which is completely
determined by its position index. Because these embeddings don't depend on
the actual input, it is implemented by simply initializing a matrix of
weights.
}
\section{Shape}{


  Inputs:

  No input tensors. Optional input parameter to limit number of positions
  (tokens) considered.

  Output:

  - \eqn{(max_position_embeddings, *, embedding_size)}
}

\examples{
emb_size <- 3L
mpe <- 2L
model <- position_embedding(embedding_size = emb_size,
                            max_position_embeddings = mpe)
model(seq_len_cap = 1)
model()
}
