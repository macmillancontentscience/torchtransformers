% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/embeddings.R
\name{position_embedding}
\alias{position_embedding}
\title{Create Position Embeddings}
\usage{
position_embedding(embedding_size, max_position_embeddings)
}
\arguments{
\item{embedding_size}{Integer; the size of the embedding space.}

\item{max_position_embeddings}{Integer; the maximum number of positions
supported.}
}
\description{
Position embeddings are how BERT-like language models represent the order of
input tokens. Each token gets a position embedding vector which is completely
determined by its position index. Because these embeddings don't depend on
the actual input, it is implemented by simply initializing a matrix of
weights.
}
\section{Shape}{


  Inputs:

  No input tensors. Optional input parameter to limit number of positions
  (tokens) considered.

  Output:

  - \eqn{(max_position_embeddings, *, embedding_size)}
}

\examples{

}
